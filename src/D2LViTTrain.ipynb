{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4332ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da792ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=96, patch_size=16, num_hiddens=512):\n",
    "        super().__init__()\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\n",
    "                                  stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Output shape: (batch size, no. of patches, no. of channels)\n",
    "        return self.conv(X).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdd8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(\n",
    "            self.dense1(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58aa2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(norm_shape)\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.ln2 = nn.LayerNorm(norm_shape)\n",
    "        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        X = self.ln1(X)\n",
    "        return X + self.mlp(self.ln2(\n",
    "            X + self.attention(X, X, X, valid_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997d897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(d2l.Classifier):\n",
    "    \"\"\"Vision transformer.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,\n",
    "                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\n",
    "                 use_bias=False, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            img_size, patch_size, num_hiddens)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\n",
    "        num_steps = self.patch_embedding.num_patches + 1  # Add the cls token\n",
    "        # Positional embeddings are learnable\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_steps, num_hiddens))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", ViTBlock(\n",
    "                num_hiddens, num_hiddens, mlp_num_hiddens,\n",
    "                num_heads, blk_dropout, use_bias))\n",
    "        self.head = nn.Sequential(nn.LayerNorm(num_hiddens),\n",
    "                                  nn.Linear(num_hiddens, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embedding(X)\n",
    "        X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
    "        X = self.dropout(X + self.pos_embedding)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X)\n",
    "        return self.head(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc2d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_dataset import TreeDataset\n",
    "import model as m\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759c632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2000 Val size: 1000\n",
      "Identified CUDA device: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_set = TreeDataset(os.path.join('..', 'data', 'trainset4'), preprocess) \n",
    "val_set = TreeDataset(os.path.join('..', 'data', 'trainset1'), preprocess)\n",
    "print(f'Train size: {len(train_set)} Val size: {len(val_set)}')\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "device = m.get_device()\n",
    "config = {'labels_key': 'digit_labels'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c230830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chami\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "img_size, patch_size = 224, 16\n",
    "num_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2\n",
    "emb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1\n",
    "model = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\n",
    "            num_blks, emb_dropout, blk_dropout, lr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2835f716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2003,  0.8277, -0.0792,  0.1954,  0.0097,  0.3583, -0.0675, -0.6295,\n",
       "         -0.1205,  0.0465]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.unsqueeze(train_set[0]['image'], 0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9870bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (conv): Conv2d(1, 512, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blks): Sequential(\n",
       "    (0): ViTBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (attention): DotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ViTMLP(\n",
       "        (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ViTBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (attention): DotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ViTMLP(\n",
       "        (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b49663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done, train loss: 0.0092 val acc: 1.0000\n",
      "Epoch 20 done, train loss: 0.0024 val acc: 1.0000\n",
      "Epoch 30 done, train loss: 0.0011 val acc: 1.0000\n",
      "Epoch 40 done, train loss: 0.0006 val acc: 1.0000\n",
      "Epoch 50 done, train loss: 0.0004 val acc: 1.0000\n",
      "Epoch 60 done, train loss: 0.0004 val acc: 1.0000\n",
      "Epoch 70 done, train loss: 0.0003 val acc: 1.0000\n",
      "Epoch 80 done, train loss: 0.0002 val acc: 1.0000\n",
      "Epoch 90 done, train loss: 0.0001 val acc: 1.0000\n",
      "Epoch 100 done, train loss: 0.0001 val acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "m.train(model, 0.0001, 0, 100, train_loader, val_loader, device, os.path.join('..', 'models', 'd2lvit'), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a75be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_acc = m.predict(model, train_loader, device, config, None)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b536134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "val_acc = m.predict(model, val_loader, device, config, None)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca491ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
